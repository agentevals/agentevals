# Tool Trajectory Evaluator Example
# Demonstrates evaluating agent tool usage patterns

name: research-workflow
version: "1.0"
description: |
  Evaluates that agents follow correct research workflows
  using appropriate tools in the right sequence.

metadata:
  author: agentevals
  tags: [example, tool-trajectory, agentic]

execution:
  evaluators:
    # Verify tools are used in correct order
    - name: workflow_order
      type: tool_trajectory
      mode: in_order
      expected:
        - tool: search
          args: any
        - tool: read_document
          args: any
        - tool: summarize
          args: any

    # Verify minimum tool usage
    - name: tool_coverage
      type: tool_trajectory
      mode: any_order
      minimums:
        search: 1
        read_document: 2

    # Performance bounds
    - name: efficiency
      type: execution_metrics
      max_tool_calls: 10
      max_duration_ms: 60000

evalcases:
  - id: research-topic
    expected_outcome: |
      Agent searches for information, reads relevant documents,
      and provides a summary. Tools should be used in logical order.

    input:
      - role: system
        content: |
          You are a research assistant with access to these tools:
          - search(query): Search for documents
          - read_document(id): Read a document by ID
          - summarize(content): Summarize content
      - role: user
        content: "Research the history of the Python programming language"

    execution:
      evaluators:
        - name: workflow
          type: tool_trajectory
          mode: in_order
          expected:
            - tool: search
              args:
                query: any  # Accept any query
            - tool: read_document
            - tool: summarize

  - id: multi-source-research
    expected_outcome: |
      Agent consults multiple sources before synthesizing answer.

    input:
      - role: user
        content: "Compare Python and JavaScript for web development"

    execution:
      evaluators:
        - name: multiple_sources
          type: tool_trajectory
          mode: any_order
          minimums:
            search: 2
            read_document: 3

        - name: response_quality
          type: llm_judge
          prompt: |
            Did the agent provide a balanced comparison?

            Response: {{candidate_answer}}

            Check:
            1. Discusses both languages
            2. Covers multiple aspects (syntax, ecosystem, performance)
            3. Provides balanced pros/cons

  - id: efficient-lookup
    expected_outcome: |
      Agent finds answer efficiently without excessive tool calls.

    input:
      - role: user
        content: "What is the current Python version?"

    execution:
      evaluators:
        - name: efficiency
          type: execution_metrics
          max_tool_calls: 3
          max_duration_ms: 10000

        - name: accuracy
          type: llm_judge
          prompt: |
            Is the answer accurate and current?
            Response: {{candidate_answer}}
